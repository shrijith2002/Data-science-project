{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLBlRbTP4Gb"
      },
      "source": [
        "Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQrUHRgqPwjz"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo pandas numpy matplotlib seaborn scikit-learn tensorflow xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JTrbhVXP9if"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY88GMfxP-YC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from ucimlrepo import fetch_ucirepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP4f3oHlQErC"
      },
      "source": [
        "Fetch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmSISFH5QFR7"
      },
      "outputs": [],
      "source": [
        "# Fetch dataset\n",
        "metro_interstate_traffic_volume = fetch_ucirepo(id=492)\n",
        "\n",
        "# Features and target\n",
        "X = metro_interstate_traffic_volume.data.features\n",
        "y = metro_interstate_traffic_volume.data.targets\n",
        "\n",
        "# Combine features and target for EDA\n",
        "data = pd.concat([X, y], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZlIfaIQKZI"
      },
      "source": [
        "Display Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN7neqmcQLG5"
      },
      "outputs": [],
      "source": [
        "print(metro_interstate_traffic_volume.metadata)\n",
        "print(metro_interstate_traffic_volume.variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrkOBC1GQQ1g"
      },
      "source": [
        "Check for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-8i5JCVQRvM"
      },
      "outputs": [],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lam47AB0Tq-l"
      },
      "outputs": [],
      "source": [
        "\n",
        "data['holiday'] = data['holiday'].fillna('None')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqtGyJeDQW66"
      },
      "source": [
        "Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA6UWqHmQXwk"
      },
      "outputs": [],
      "source": [
        "# Handle Missing Values for numeric columns only\n",
        "for column in data.select_dtypes(include=np.number).columns:\n",
        "    data.loc[:, column] = data[column].fillna(data[column].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyGi4mqCTxgJ"
      },
      "outputs": [],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvgMcS0MQ7Af"
      },
      "source": [
        "Exploratory Data Analysis (EDA) - Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MDdUxfUQ8na"
      },
      "outputs": [],
      "source": [
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwkdaWLQ-Ar"
      },
      "source": [
        "EDA - Traffic Volume Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZAqYpoCRAUz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['traffic_volume'], bins=50, kde=True)\n",
        "plt.title('Traffic Volume Distribution')\n",
        "plt.xlabel('Traffic Volume')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_UWgtjURGS5"
      },
      "source": [
        "EDA - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b31NnhORHK6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "# Calculate correlation only for numeric columns\n",
        "numeric_data = data.select_dtypes(include=np.number)\n",
        "sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEgO6Z5oV9sf"
      },
      "source": [
        "As dataset seems too much imbalanced and missing values we will now clean it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgOT5LgKRd7M"
      },
      "source": [
        "Feature Engineering - Extract Time-Based Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBHgO0fZRemX"
      },
      "outputs": [],
      "source": [
        "data['date_time'] = pd.to_datetime(data['date_time'])\n",
        "data['hour'] = data['date_time'].dt.hour\n",
        "data['day_of_week'] = data['date_time'].dt.dayofweek\n",
        "data['month'] = data['date_time'].dt.month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bndgDbWRRijo"
      },
      "source": [
        "Feature Engineering - Encode Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ1Hyp-PRjg_"
      },
      "outputs": [],
      "source": [
        "data = pd.get_dummies(data, columns=['weather_main', 'holiday'], drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJh1g2fzRnQD"
      },
      "source": [
        "Drop Unnecessary Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMR-dlqkRnzk"
      },
      "outputs": [],
      "source": [
        "data.drop(['date_time'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFSr2vVRsWa"
      },
      "source": [
        " Split Data into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8pm1ps0Rs_f"
      },
      "outputs": [],
      "source": [
        "X = data.drop('traffic_volume', axis=1)\n",
        "y = data['traffic_volume']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYFftcnhRxXj"
      },
      "source": [
        "Normalize Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX0S7kGeRyE9"
      },
      "outputs": [],
      "source": [
        "metro_interstate_traffic_volume = fetch_ucirepo(id=492)\n",
        "X = metro_interstate_traffic_volume.data.features\n",
        "# The target variable 'y' should be assigned to the 'targets' attribute.\n",
        "y = metro_interstate_traffic_volume.data.targets\n",
        "data = pd.concat([X, y], axis=1)\n",
        "encoded_data = pd.get_dummies(data, columns=['weather_main', 'holiday', 'weather_description'], drop_first=True)\n",
        "encoded_data.drop(['date_time'], axis=1, inplace=True)\n",
        "#  Split Data into Training and Testing Sets\n",
        "X = encoded_data.drop('traffic_volume', axis=1)\n",
        "y = encoded_data['traffic_volume']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize Numerical Features\n",
        "# Now apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6B9ulFNVIg2"
      },
      "outputs": [],
      "source": [
        "data['holiday'] = data['holiday'].fillna('None')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5d5D_XAVOo8"
      },
      "outputs": [],
      "source": [
        "data['date_time'] = pd.to_datetime(data['date_time'])\n",
        "data['hour'] = data['date_time'].dt.hour\n",
        "data['day_of_week'] = data['date_time'].dt.dayofweek  # 0: Monday, 6: Sunday\n",
        "data['month'] = data['date_time'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwfzqkFcVUEB"
      },
      "outputs": [],
      "source": [
        "data = pd.get_dummies(data, columns=['weather_main', 'weather_description', 'holiday'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afjltd3sVXLX"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or9BIkH0Vkk9"
      },
      "outputs": [],
      "source": [
        "data.drop(['date_time'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWwY7HWVthG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = data.drop('traffic_volume', axis=1)\n",
        "y = data['traffic_volume']\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features and transform them\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert the scaled features back to a DataFrame\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akXAXI99WL5S"
      },
      "source": [
        "Model 1 Random Forest Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxZryQizV5n1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print('Random Forest MAE:', mean_absolute_error(y_test, y_pred))\n",
        "print('Random Forest RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print('Random Forest R2:', r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5szvjZMTFty"
      },
      "source": [
        "Evaluate Random Forest Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zvfaqrlTKw1"
      },
      "source": [
        "XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB1RrCURTNIa"
      },
      "outputs": [],
      "source": [
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqzUY26uWexh"
      },
      "outputs": [],
      "source": [
        "print('XGBoost MAE:', mean_absolute_error(y_test, y_pred_xgb))\n",
        "print('XGBoost RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
        "print('XGBoost R2:', r2_score(y_test, y_pred_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6atOMYrW1yZ"
      },
      "source": [
        "Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDBQ-FDSW4te"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=XGBRegressor(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           cv=5,\n",
        "                           verbose=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_xgb_model = XGBRegressor(**best_params, random_state=42)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "print('XGBoost MAE:', mean_absolute_error(y_test, y_pred_xgb))\n",
        "print('XGBoost RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_xgb)))\n",
        "print('XGBoost R2:', r2_score(y_test, y_pred_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU3eB5T7XKQ7"
      },
      "source": [
        "LSTM Model Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46hjdeNwXMTr"
      },
      "outputs": [],
      "source": [
        "X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_lstm = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPXPKQ8nXwYG"
      },
      "source": [
        "Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seEL9vZDXxHR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "# Build LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Input(shape=(1, X_train.shape[1])))\n",
        "lstm_model.add(LSTM(50, activation='relu'))\n",
        "lstm_model.add(Dense(1))\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvXtpE_6YCUW"
      },
      "source": [
        "Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw-1-uMVYC2y"
      },
      "outputs": [],
      "source": [
        "history = lstm_model.fit(X_train_lstm, y_train, epochs=50, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-azYJprWYJae"
      },
      "source": [
        "Plot LSTM Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4shPR_-YKTV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKofBxmSYRWH"
      },
      "source": [
        "Evaluate LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcykPn3CYSRt"
      },
      "outputs": [],
      "source": [
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "print('LSTM MAE:', mean_absolute_error(y_test, y_pred_lstm))\n",
        "print('LSTM RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_lstm)))\n",
        "print('LSTM R2:', r2_score(y_test, y_pred_lstm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1KFb0RjYU_Q"
      },
      "source": [
        "Compare Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_9dyAlYxhV"
      },
      "outputs": [],
      "source": [
        "models = ['Random Forest', 'XGBoost', 'LSTM']\n",
        "mae_scores = [mean_absolute_error(y_test, y_pred), mean_absolute_error(y_test, y_pred_xgb), mean_absolute_error(y_test, y_pred_lstm)]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=models, y=mae_scores)\n",
        "plt.title('Model Comparison - MAE')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJAmuOiLYzm8"
      },
      "source": [
        "Feature Importance - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtUZE2__Y2aR"
      },
      "outputs": [],
      "source": [
        "importances = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=importances, y=feature_names)\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}