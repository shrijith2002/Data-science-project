# -*- coding: utf-8 -*-
"""Traffic_Flow_Pred2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuBvvcr3Oro0YhriibGilmT7Wn82juaj

Install Required Libraries
"""

!pip install ucimlrepo pandas numpy matplotlib seaborn scikit-learn tensorflow xgboost

"""Import Libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from ucimlrepo import fetch_ucirepo

"""Fetch Dataset"""

# Fetch dataset
metro_interstate_traffic_volume = fetch_ucirepo(id=492)

# Features and target
X = metro_interstate_traffic_volume.data.features
y = metro_interstate_traffic_volume.data.targets

# Combine features and target for EDA
data = pd.concat([X, y], axis=1)

"""Display Dataset Information"""

print(metro_interstate_traffic_volume.metadata)
print(metro_interstate_traffic_volume.variables)

"""Check for Missing Values"""

print(data.isnull().sum())

data['holiday'] = data['holiday'].fillna('None')

"""Handle Missing Values"""

# Handle Missing Values for numeric columns only
for column in data.select_dtypes(include=np.number).columns:
    data.loc[:, column] = data[column].fillna(data[column].median())

print(data.isnull().sum())

"""Exploratory Data Analysis (EDA) - Summary Statistics"""

print(data.describe())

"""EDA - Traffic Volume Distribution"""

plt.figure(figsize=(10, 6))
sns.histplot(data['traffic_volume'], bins=50, kde=True)
plt.title('Traffic Volume Distribution')
plt.xlabel('Traffic Volume')
plt.ylabel('Frequency')
plt.show()

"""EDA - Correlation Heatmap"""

plt.figure(figsize=(12, 8))
# Calculate correlation only for numeric columns
numeric_data = data.select_dtypes(include=np.number)
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""As dataset seems too much imbalanced and missing values we will now clean it

Feature Engineering - Extract Time-Based Features
"""

data['date_time'] = pd.to_datetime(data['date_time'])
data['hour'] = data['date_time'].dt.hour
data['day_of_week'] = data['date_time'].dt.dayofweek
data['month'] = data['date_time'].dt.month

"""Feature Engineering - Encode Categorical Variables"""

data = pd.get_dummies(data, columns=['weather_main', 'holiday'], drop_first=True)

"""Drop Unnecessary Columns"""

data.drop(['date_time'], axis=1, inplace=True)

""" Split Data into Training and Testing Sets"""

X = data.drop('traffic_volume', axis=1)
y = data['traffic_volume']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Normalize Numerical Features"""

metro_interstate_traffic_volume = fetch_ucirepo(id=492)
X = metro_interstate_traffic_volume.data.features
# The target variable 'y' should be assigned to the 'targets' attribute.
y = metro_interstate_traffic_volume.data.targets
data = pd.concat([X, y], axis=1)
encoded_data = pd.get_dummies(data, columns=['weather_main', 'holiday', 'weather_description'], drop_first=True)
encoded_data.drop(['date_time'], axis=1, inplace=True)
#  Split Data into Training and Testing Sets
X = encoded_data.drop('traffic_volume', axis=1)
y = encoded_data['traffic_volume']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize Numerical Features
# Now apply StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

data['holiday'] = data['holiday'].fillna('None')

data['date_time'] = pd.to_datetime(data['date_time'])
data['hour'] = data['date_time'].dt.hour
data['day_of_week'] = data['date_time'].dt.dayofweek  # 0: Monday, 6: Sunday
data['month'] = data['date_time'].dt.month

data = pd.get_dummies(data, columns=['weather_main', 'weather_description', 'holiday'], drop_first=True)

data.head()

data.drop(['date_time'], axis=1, inplace=True)

from sklearn.preprocessing import StandardScaler

# Separate features (X) and target (y)
X = data.drop('traffic_volume', axis=1)
y = data['traffic_volume']

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler to the features and transform them
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame
X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)